{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Physical Informality Of Built Environment Through Earth Observation And Deep Learning\n\n### An ongoing collaborative efforts towards open-source platform\n\n## Foreword\n-------------------\nUsing satellite imagery data for mapping informal settlements or slums has been active for years. The slums are characterized by deprivation in multiple dimensions, such as poverty, lacking land and housing tenure, as well as basic services. But in satellite imagery data, only those physical characteristics can be captured. Thus, to use satellite images, there is a fundamental assumption of physical manifestation of the multidimensional deprivation, which means socioeconomic status such as poverty is manifested in the physical structure of the settlements. Then, how to measure the physical structure to differentiate slums?\n\nConventional computer vision and machine learning techniques depends on prior knowledge about *image features*. It means that we human beings are supposed to know some *image features* such as lines and edges (or other higher order features) for deriving statistics such as line length and direction homogeneity, and ultimately differentiating slums. This is not always the case in reality as physical manifestation of socioeconomic status can be very subtle and properly characterized by few known features. It's now a natural transition from conventional machine learning to deep learning, which is known as *representation learning* for its capability of extracting features automatically. From the optimistic side, deep learning would always return *image feature(s)*, even subtle, if existed. On the flip side, we would not expect deep learning can differentiate slums which are physically equivalent to non-slums. Then, the problem seems to reduce to labelling--we only would like to train the model by using slums with at least \"some\" physical manifestation of deprivation other than the phyiscal ones. In this sense, we would like to stay conservative to focus on physical informality. But still, even if we constrain ourself to the physical charactertistics of slums, there is a risk of *informality out of slum\"* during prediction. What if the trained model \"sees\" similar physical informality outside of slums? In fact, this is highly possible if the model is only trained by physical characteristics without being provided with land use information. The strength of deep learning at the level of mapping *land cover* physical characteristics by no means to yield lifted expectation at another level in mapping *land use* types. Thus we recognize the *informality out of slum\"* as a natural output as opposed to the conventional *false positive*, and finally arrive at our title of ***physical informality of built environment***, which means we are still confident that deep learning can more or less differentiate phyical characteristics between *built* and *non-built* in the sense of *land use*.\n\nIn this project, we set out with the following ultimate goals while using deep learning techniques for physical informality mapping:\n- focusing on model simplicity and generalization;\n- focusing on training data representativeness (what are the representative training areas);\n- ensuring consistency and reproducibility.\n\n## Technical backends\n-------------------\n##### This page contains a Python program for mapping physical informality of built environment by using deep learning technique and earth observatory datasets.\n\n##### The program is running in the form of Python Jupyter Notebook based on designated kernel served by Kaggle.com, where kernels are standalone programming environment . The standalone kernel along with the Jupyter Notebook format enforce the program to be in consistent environment and executed in controlled sequence, and further ensure experiment reproducibility.\n\n##### The Python Jupyter Notebook also ensures sectional execution of the program and gives a feeling of \"click-and-run\".\n\n##### The program is still under development and subject to modification before publishing.\n\n-------------------\n###### Credit to this work can be given as:\n```\nWang,J, Kuffer,M, Physical informality of built environment through deep neural networks, (2020), Kaggle community,\nhttps://www.kaggle.com/jonwang4/deepslum\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------\n# Program Name:        Deep Convolutional Neural Network (DCNN) for built informality mapping\n# Purpose:     Test DCNN on mapping degrees of deprivation\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# Input data files are available in the read-only \"../input/\" directory\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Version:     0.1 \n#              Functionalities:\n#              1. Loading data;\n#              2. Preparing training and testing sets;\n#              3. Setting up experiment, including model configuration and training session;\n#              4. Predicting.\n\n# Author:      Jiong (Jon) Wang\n#\n# Created:     26/05/2020\n# Copyright:   (c) JonWang 2020\n# Licence:     <your licence>\n#-------------------------------------------------------------------------------\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, model_from_json, load_model\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\nfrom tensorflow.keras.layers import (\n    Conv2D,\n    UpSampling2D,\n    MaxPooling2D,\n    Input,\n    Conv2DTranspose,\n    Flatten,\n    BatchNormalization,\n    Activation,\n    concatenate\n)\nfrom tensorflow.keras.layers import RepeatVector, Reshape\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom scipy import interpolate\nfrom osgeo import gdal_array\nfrom pathlib import Path\nfrom functools import partial\nfrom sklearn.metrics import jaccard_score\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os\nimport tensorflow as tf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 00 View files and versions\n##############################################\n\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint('\\nUsing tensorflow version %s' % (tf.__version__))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 01 Load and organize images\n##############################################\n\n# load rasters: base image along with its label\n# Stack base image and label into a list of array\ndef load_rasters(path, subUL, band_ind):  # Subset from original raster with extent and upperleft coord\n    \"\"\"Load training data pairs (two high resolution images and two low resolution images)\"\"\"\n    file_list = []  # List image name\n    for file in Path(path).glob('*.tif'):\n        file_list.append(file)\n    assert len(file_list) == 2\n\n    # Ensure the order of the list: base image first !!\n    for file in file_list:  # Organize file list\n        img_name = file.name\n        if 'base' in img_name:\n            base = file\n        elif 'label' in img_name:\n            label = file\n    file_list = [base, label]\n    \n    stack = []  # Stack base and label together into a 3D array\n    for file in file_list:\n        if file.name.startswith('base'):\n            data = gdal_array.LoadFile(str(file), xoff=subUL[0], yoff=subUL[1]) #.astype(np.int),ysize=extent[1],xsize=extent[0]\n            data = data[tuple(band_ind),:,:]  # Worldview image with 3rd dimension at first\n            data = np.transpose(data,(1,2,0))  # Transpose 3rd to last \n            stack.append(data)\n        else:\n            data = gdal_array.LoadFile(str(file), xoff=subUL[0], yoff=subUL[1]) #.astype(np.int),xsize=extent[0],ysize=extent[1]\n            data = data[:,:,np.newaxis]\n            stack.append(data)\n#        image = Image.fromarray(data)\n#        data = nan_remover(data)\n#        setattr(image, 'filename', file)\n    # Ensure the size of base and label is are consistent\n    assert stack[0].shape[0] == stack[-1].shape[0]\n    assert stack[0].shape[1] == stack[-1].shape[1]\n    return stack[:-1], stack[-1]\n\n\n# Clean the NaN values\ndef nan_remover(array):\n    x = np.arange(0, array.shape[1])\n    y = np.arange(0, array.shape[0])\n    # Masking invalid values\n    array = np.ma.masked_invalid(array)\n    xx, yy = np.meshgrid(x, y)\n    # Getting only the valid values\n    x1 = xx[~array.mask]\n    y1 = yy[~array.mask]\n    newarr = array[~array.mask]\n    array_interp = interpolate.griddata((x1, y1), newarr.ravel(),\n                              (xx, yy), method='nearest')\n    # Clean the edge\n    bad_indexes = np.isnan(array_interp)\n    good_indexes = np.logical_not(bad_indexes)\n    good_data = array_interp[good_indexes]\n    interpolated = np.interp(bad_indexes.nonzero()[0], good_indexes.nonzero()[0], good_data)\n    array_interp[bad_indexes] = interpolated\n    return array_interp\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 02 Prepare data for train, val, test\n##############################################\n\n# Sample patches from an image band/layer\n# Stride controls the overlap of patches\ndef gen_patches(image, size, stride):\n    \"\"\"Segment input images into patches\"\"\"\n    if not isinstance(size, tuple):  # Ensure format is tuple\n        size = (size, size)\n    if stride is None:\n        stride = size\n    elif not isinstance(stride, tuple):\n        stride = (stride, stride)\n    # Columns in priority\n    for i in range(0, image.shape[0] - size[0] + 1, stride[0]):  # One patch every stride\n        for j in range(0, image.shape[1] - size[1] + 1, stride[1]):\n            yield image[i:i + size[0], j:j + size[1], :]  # If Pillow Image is used: image.crop([i, j, i + size[0], j + size[1]])\n\n\n# Advanced version of patch sampling\n# Sample patches at target areas with criteria, such as label size\ndef gen_patches_ctrl():  # With controlled position\n    \n    return None\n\n\n# Generate patches for all layers/bands in stack\ndef stack_to_patches(stack, size, stride, patches):\n#    assert len(stack) == 4\n    for i in range(len(stack)):  # Loop over the layers/bands in the stack\n        # If Pillow Image: img_to_array(img)\n        patches[i] += [img.astype('float32') for img in gen_patches(stack[i], size, stride)]\n\n\n# Arrange training and validation sets from the patches\ndef load_train_set(data_dir, subUL, band_ind, size, stride):\n    # Load image data from training folder\n    patches = [[] for _ in range(2)]  # Empty list to store patches for each layer/band in stack\n    for train_path in Path(data_dir/'train').glob('*'):  # Loop over all folders\n        if train_path.is_dir():\n            print('loading image pairs from {}'.format(train_path))\n            stack = load_rasters(train_path, subUL, band_ind)\n            stack = [*stack[0], stack[1]]\n            # subset samples into patches\n            stack_to_patches(stack, size, stride, patches)\n            \n    # Split patches into training and validation sets        \n    patch_train = [[] for _ in range(2)]\n    patch_val = [[] for _ in range(2)]\n    for i in range(2):\n        patch_train[i] = np.stack(patches[i][:int(len(patches[i])*0.7)])\n        patch_val[i] = np.stack(patches[i][int(len(patches[i])*0.7):])\n    # Return 4-dimensional array (number, height, width, channel)\n    return patch_train[:-1], patch_train[-1], patch_val[:-1], patch_val[-1]\n\n\n# Arrange test set by using another set of raster input\ndef load_test_set(stack, block_size):\n    assert len(stack) == 2    \n    stack = [*stack[0], stack[1]]  # Update stack by split tuple into list\n    patches = [[] for _ in range(len(stack))]  # Stack length already changed\n    stack_to_patches(stack, size=block_size, stride=None, patches=patches)\n\n    for i in range(len(stack)):\n        patches[i] = np.stack(patches[i])\n    return patches[:-1], patches[-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 03 Customize loss function metrics\n##############################################\n\n# Jaccard index realized as intersection over union (iou)\ndef mean_iou(y_true, y_pred):\n    # Consider prediction greater than 0.5\n    y_pred = K.cast(K.greater(y_pred, .5), dtype='float32') # .5 is the threshold\n    inter = K.sum(K.sum(K.squeeze(y_true * y_pred, axis=3), axis=2), axis=1)\n    union = K.sum(K.sum(K.squeeze(y_true + y_pred, axis=3), axis=2), axis=1) - inter\n    return K.mean((inter + K.epsilon()) / (union + K.epsilon()))\n\n\n# Covariance\ndef cov(y_true, y_pred):\n    return K.mean((y_true - K.mean(y_true)) * K.transpose((y_pred - K.mean(y_pred))))\n\n\n# Correlation\ndef r2(y_true, y_pred):\n    # mean calls tensor property instead of ndarray\n    tf_true = y_true\n    if not isinstance(y_true, tf.Tensor):\n        tf_true = tf.convert_to_tensor(y_true)\n    res = K.sum(K.square(y_true - y_pred))\n    tot = K.sum(K.square(y_true - K.mean(tf_true)))\n    return 1 - res / (tot + K.epsilon())\n\n\n# Signal-to-noise ratio\ndef psnr(y_true, y_pred, data_range=50):\n    #Peak signal-to-noise ratio averaged over samples and channels\n    mse = K.mean(K.square(y_true - y_pred), axis=(-3, -2))\n    return K.mean(20 * K.log(data_range / K.sqrt(mse)) / np.log(10))\n\n\n# structural similarity measurement system\ndef ssim(y_true, y_pred, data_range=50):\n    \"\"\"structural similarity measurement system.\"\"\"\n    K1 = 0.01\n    K2 = 0.03\n\n    mu_x = K.mean(y_pred)\n    mu_y = K.mean(y_true)\n\n    sig_x = K.std(y_pred)\n    sig_y = K.std(y_true)\n    sig_xy = cov(y_true, y_pred)\n\n    L = data_range\n    C1 = (K1 * L) ** 2\n    C2 = (K2 * L) ** 2\n\n    return ((2 * mu_x * mu_y + C1) * (2 * sig_xy * C2) /\n            (mu_x ** 2 + mu_y ** 2 + C1) * (sig_x ** 2 + sig_y ** 2 + C2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 04 Convolutional Neural Networks\n##############################################\n\n# U-net \ndef u_net(img):\n    inputs = Input(shape=img.shape[-3:])\n    conv1_1 = Conv2D(16, (3, 3), padding='same')(inputs)\n    bn1_1 = BatchNormalization(axis=3)(conv1_1)\n    relu1_1 = Activation('relu')(bn1_1)\n    conv1_2 = Conv2D(16, (3, 3), padding='same')(relu1_1)\n    bn1_2 = BatchNormalization(axis=3)(conv1_2)\n    relu1_2 = Activation('relu')(bn1_2)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(relu1_2)\n    \n    conv2_1 = Conv2D(32, (3, 3), padding='same')(pool1)\n    bn2_1 = BatchNormalization(axis=3)(conv2_1)\n    relu2_1 = Activation('relu')(bn2_1)\n    conv2_2 = Conv2D(32, (3, 3), padding='same')(relu2_1)\n    bn2_2 = BatchNormalization(axis=3)(conv2_2)\n    relu2_2 = Activation('relu')(bn2_2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(relu2_2)\n    \n    conv3_1 = Conv2D(64, (3, 3), padding='same')(pool2)\n    bn3_1 = BatchNormalization(axis=3)(conv3_1)\n    relu3_1 = Activation('relu')(bn3_1)\n    conv3_2 = Conv2D(64, (3, 3), padding='same')(relu3_1)\n    bn3_2 = BatchNormalization(axis=3)(conv3_2)\n    relu3_2 = Activation('relu')(bn3_2)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(relu3_2)\n    \n    conv4_1 = Conv2D(128, (3, 3), padding='same')(pool3)\n    bn4_1 = BatchNormalization(axis=3)(conv4_1)\n    relu4_1 = Activation('relu')(bn4_1)\n    conv4_2 = Conv2D(128, (3, 3), padding='same')(relu4_1)\n    bn4_2 = BatchNormalization(axis=3)(conv4_2)\n    relu4_2 = Activation('relu')(bn4_2)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(relu4_2)\n    \n    conv5_1 = Conv2D(256, (3, 3), padding='same')(pool4)\n    bn5_1 = BatchNormalization(axis=3)(conv5_1)\n    relu5_1 = Activation('relu')(bn5_1)\n    conv5_2 = Conv2D(256, (3, 3), padding='same')(relu5_1)\n    bn5_2 = BatchNormalization(axis=3)(conv5_2)\n    relu5_2 = Activation('relu')(bn5_2)\n    \n    up6 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(relu5_2), relu4_2], axis=3)\n    conv6_1 = Conv2D(128, (3, 3), padding='same')(up6)\n    bn6_1 = BatchNormalization(axis=3)(conv6_1)\n    relu6_1 = Activation('relu')(bn6_1)\n    conv6_2 = Conv2D(128, (3, 3), padding='same')(relu6_1)\n    bn6_2 = BatchNormalization(axis=3)(conv6_2)\n    relu6_2 = Activation('relu')(bn6_2)\n    \n    up7 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(relu6_2), relu3_2], axis=3)\n    conv7_1 = Conv2D(64, (3, 3), padding='same')(up7)\n    bn7_1 = BatchNormalization(axis=3)(conv7_1)\n    relu7_1 = Activation('relu')(bn7_1)\n    conv7_2 = Conv2D(64, (3, 3), padding='same')(relu7_1)\n    bn7_2 = BatchNormalization(axis=3)(conv7_2)\n    relu7_2 = Activation('relu')(bn7_2)\n    \n    up8 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(relu7_2), relu2_2], axis=3)\n    conv8_1 = Conv2D(32, (3, 3), padding='same')(up8)\n    bn8_1 = BatchNormalization(axis=3)(conv8_1)\n    relu8_1 = Activation('relu')(bn8_1)\n    conv8_2 = Conv2D(32, (3, 3), padding='same')(relu8_1)\n    bn8_2 = BatchNormalization(axis=3)(conv8_2)\n    relu8_2 = Activation('relu')(bn8_2)\n    \n    up9 = concatenate([Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(relu8_2), relu1_2], axis=3)\n    conv9_1 = Conv2D(16, (3, 3), padding='same')(up9)\n    bn9_1 = BatchNormalization(axis=3)(conv9_1)\n    relu9_1 = Activation('relu')(bn9_1)\n    conv9_2 = Conv2D(16, (3, 3), padding='same')(relu9_1)\n    bn9_2 = BatchNormalization(axis=3)(conv9_2)\n    relu9_2 = Activation('relu')(bn9_2)\n    \n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(relu9_2)\n    \n    model = Model(inputs=[inputs], outputs=[conv10])\n    print(model.summary())\n    \n    return model\n\n\ndef get_model(name):\n    \"\"\"Get model function from the name space in strings\"\"\"\n    return globals()[name]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# 05 Train and predict\n##############################################\n\n# Define an experiment for training and test session\nclass Experiment(object):\n    def __init__(self, load_set, build_model, optimizer, save_dir='.'):\n        self.load_set = load_set\n        self.build_model = build_model\n        self.optimizer = optimizer\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n\n        self.config_file = self.save_dir / 'config.yaml'\n        self.model_file = self.save_dir / 'model.hdf5'\n        self.visual_file = self.save_dir / 'model.eps'\n\n        self.train_dir = self.save_dir / 'train'\n        self.train_dir.mkdir(exist_ok=True)\n        self.history_file = self.train_dir / 'history.csv'\n        self.weights_dir = self.train_dir / 'weights'\n        self.weights_dir.mkdir(exist_ok=True)\n\n        self.test_dir = self.save_dir / 'test'\n        self.test_dir.mkdir(exist_ok=True)\n\n    def weights_file(self, epoch=None):\n        if epoch is None:\n            return self.weights_dir / 'ep{epoch:04d}.hdf5'\n        else:\n            return self.weights_dir / 'ep{:04d}.hdf5'.format(epoch)\n\n    @property\n    def latest_epoch(self):\n        try:\n            return pd.read_csv(str(self.history_file))['epoch'].iloc[-1]\n        except (FileNotFoundError, pd.io.common.EmptyDataError):\n            pass\n        return -1\n\n    @staticmethod\n    def _ensure_dimension(array, dim):\n        while len(array.shape) < dim:\n            array = array[np.newaxis, ...]\n        return array\n\n    @staticmethod\n    def _ensure_channel(array, c):\n        return array[..., c:c + 4]\n\n    @staticmethod\n    def validate(array):\n        array = Experiment._ensure_dimension(array, 4)\n        array = Experiment._ensure_channel(array, 0)\n        return array\n\n    def compile(self, model):\n        \"\"\"Compile model with default settings.\"\"\"\n        model.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=[mean_iou])\n        return model\n\n    def train(self, data_dir, epochs, band_ind, resume=True):\n        # Load and process data\n        x_train, y_train, x_val, y_val = self.load_set()\n        assert len(x_train) == len(x_val)\n        for i in range(1):\n            x_train[i], x_val[i] = [self.validate(x) for x in [x_train[i], x_val[i]]]\n        y_train, y_val = [self.validate(y) for y in [y_train, y_val]]\n\n        # Compile model\n        model = self.compile(self.build_model(*x_train))\n        model.summary()\n        #self.config_file.write_text(model.to_yaml())\n        #plot_model(model, to_file=str(self.visual_file), show_shapes=False)\n\n        # Inherit weights\n        if resume:\n            latest_epoch = self.latest_epoch\n            if latest_epoch > -1:\n                weights_file = self.weights_file(epoch=latest_epoch)\n                model.load_weights(str(weights_file))\n            initial_epoch = latest_epoch + 1\n        else:\n            initial_epoch = 0\n\n        # Set up callbacks\n        callbacks = []\n        callbacks += [ModelCheckpoint(str(self.model_file))]\n#        callbacks += [ModelCheckpoint(str(self.weights_file()), save_weights_only=True)]\n        callbacks += [CSVLogger(str(self.history_file), append=resume)]\n        callbacks += [ReduceLROnPlateau(factor=0.5, cooldown=0, patience=30, min_lr=0.5e-5)]\n\n        # Train\n        model.fit(x_train, y_train, batch_size=16, epochs=epochs, callbacks=callbacks,\n                  validation_data=(x_val, y_val), initial_epoch=initial_epoch)\n\n        # Plot metrics history\n        prefix = str(self.history_file).rsplit('.', maxsplit=1)[0]\n        df = pd.read_csv(str(self.history_file))\n        epoch = df['epoch']\n        for metric in ['Loss', 'mean_iou']:\n            train = df[metric.lower()]\n            val = df['val_' + metric.lower()]\n            plt.figure()\n            plt.plot(epoch, train, label='train')\n            plt.plot(epoch, val, label='val')\n            plt.legend(loc='best')\n            plt.xlabel('Epoch')\n            plt.ylabel(metric)\n            plt.savefig('.'.join([prefix, metric.lower(), 'png']))\n            plt.close()\n\n    def test_on_image(self, image_dir, output_dir, subUL, band_ind, \n                      block_size, metrics=[jaccard_score]):\n        # Load images\n        print('Loading test image from {}'.format(image_dir))\n        input_images, valid_image = load_rasters(image_dir, subUL, band_ind)\n        assert input_images[0].shape[-1] == len(band_ind)\n        name = input_images[-1].filename.name if hasattr(input_images[-1], 'filename') else ''\n        print('Predict on image {}'.format(name))\n        \n        # Pad input image as multiple of block size\n        input_row, input_col = input_images[0].shape[0], input_images[0].shape[1]\n        input_images[0] = np.lib.pad(input_images[0], ((0, block_size[0]-input_row%block_size[0]), \n                                           (0, block_size[1]-input_col%block_size[1]),(0,0)), 'edge')\n\n        # Generate output image and measure run time\n        # The shape of the x_inputs (numbers, height, width, channels)\n        x_inputs = [self.validate(img_to_array(im)) for im in input_images]\n#        assert x_inputs[0].shape[1] % block_size[0] == 0\n#        assert x_inputs[0].shape[2] % block_size[1] == 0\n        x_train, _ = load_test_set((input_images, valid_image), block_size=block_size)\n\n        model = self.compile(self.build_model(*x_train))\n        if self.model_file.exists():\n            model.load_weights(str(self.model_file))\n\n        t_start = time.perf_counter()\n        y_preds = model.predict(x_train, batch_size=1)  # 4-dimensional array with batch size\n        # map predicted patches back to original image extent\n        y_pred = np.empty((input_images[0].shape[0], input_images[0].shape[1], 1), dtype=np.float32)\n        row_step = block_size[0]\n        col_step = block_size[1]\n        rows = x_inputs[0].shape[1] // block_size[0]\n        cols = x_inputs[0].shape[2] // block_size[1]\n        count = 0\n        for i in range(cols):\n            for j in range(rows):\n                y_pred[i * row_step: (i + 1) * row_step, j * col_step: (j + 1) * col_step] = y_preds[count]\n                count += 1\n        assert count == rows * cols\n        y_pred = y_pred[:valid_image.shape[0],:valid_image.shape[1]]  # Cut back to unpadded size\n        \n        # Plot prediction and reference\n        f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12,5))\n        ax1.imshow(y_pred[:,:,0],'gray')\n        ax1.set_title('Prediction')\n        ax2.imshow(valid_image[:,:,0],'gray')\n        ax2.set_title('Reference')\n\n        t_end = time.perf_counter()\n\n        # Record metrics\n        row = pd.Series()\n        row['name'] = name\n        row['time'] = t_end - t_start\n        y_true = self.validate(img_to_array(valid_image))\n        y_pred = self.validate(y_pred)\n        for metric in metrics:\n#            row[metric.__name__] = K.eval(metric(y_true, y_pred))\n            row[metric.__name__] = metric(y_true[0].squeeze(), \n               (y_pred[0].squeeze()>.5).astype(int), average='macro')\n\n        prototype = str(valid_image.filename) if hasattr(valid_image, 'filename') else None\n        gdal_array.SaveArray(y_pred[0].squeeze().astype(np.int16),\n                             str(output_dir / name),\n                             prototype=prototype)\n        return row\n    \n    def test(self, data_dir, subUL, band_ind, block_size=(500, 500), metrics=[jaccard_score]):\n        test_set='test'\n        print('Testing...')\n        output_dir = self.test_dir/test_set\n        output_dir.mkdir(exist_ok=True)\n\n        # Evaluate metrics on each image\n        # Different from training that load all images at once before training\n        # test_on_image is put in the loop called for each image\n        rows = []\n        for image_path in Path(data_dir/test_set).glob('*'):\n            if image_path.is_dir():\n                rows += [self.test_on_image(image_path, output_dir, subUL, band_ind, \n                                            block_size=block_size, metrics=metrics)]\n        df = pd.DataFrame(rows)\n        # Compute average metrics\n        row = pd.Series()\n        row['name'] = 'average'\n        for col in df:\n            if col != 'name':\n                row[col] = df[col].mean()\n        df = df.append(row, ignore_index=True)\n        df.to_csv(str(self.test_dir / '{}/metrics.csv'.format(test_set)))\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################\n# Manual run\n##############################################\n\n#------------------------\n# Set working directory and parameters\n#------------------------\n\n# Working directory\n#repo_dir = Path('__file__').parents[0]\ndata_dir = Path('../input/bangalore/sample_data')\nsave_dir = Path('../output/kaggle/working')\n\n# Affiliated parameters from JSON file\nwith open('../input/bangalore/parameter.json', 'r') as read_file:\n    param = json.load(read_file)\n\nfrom ipywidgets import widgets, interact, HTML, HBox, VBox\nfrom IPython.display import display\n\npatch_size = widgets.SelectionSlider(options=[32,64,128,256], value=32,\n                              description='Patch Size')\npatch_stride = widgets.IntSlider(min=1, max=32, step=1, value=4, description='Patch Stride')\nbands = widgets.Select(options = {'NIR-RGB':[7,5,3,2], 'NIR-RGB2':[8,5,3,2]}, \n                          value=[7,5,3,2], description='Band Index')\nepoch = widgets.IntSlider(min=10, max=200, step=10, value=10, \n                           description='Training Epochs')\nlearn_rate = widgets.FloatLogSlider(base=10, min=-4, max=-2, step=0.5, value=0.0001, \n                            description='Learning Rate')\nlearn_decay = widgets.FloatLogSlider(base=10, min=-8, max=-3, step=0.5, value=0.00001, \n                               description='Learning Rate Decay')\ntitle = HTML('<h3 class=\"text-center\">Set <font color=\"green\">Parameters</font><h3>')\n\ncont = HBox([VBox([bands],height='100px',width='50%'), \n             VBox([patch_size, patch_stride, learn_rate, learn_decay, epoch],height='100px',width='50%')])\ndisplay(title, cont)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------\n# Experiment configure and compile\n#------------------------\n\n# Input training patch dimensions\nsize=patch_size.value #param['size']  # img_rows, img_cols = 32, 32\nstride=patch_stride.value  #param['stride']  # Sampling stride\n#    extent = param['extent']\nepochs=epoch.value  #param['epochs']\n# Index of selected band\nband_ind=bands.value  #param['band_ind']\n# Subset study area\nsubUL=param['subUL']\nblock_size=tuple(param['block_size'])\n\n\nbuild_model = get_model(param['model']['name'])\nif 'optimizer' in param:\n    optimizer = getattr(optimizers, param['optimizer']['name'])\n    optimizer = optimizer(lr=learn_rate.value, decay=learn_decay.value)  #(**param['optimizer']['params'])\nelse:\n    optimizer = 'Adam'\n       \n# Simple version of data loading functionality\nload_set = partial(load_train_set, data_dir, \n                   subUL, band_ind, size, stride)\n\n# Setup experiment\nexpt = Experiment(load_set=load_set,\n                  build_model=build_model, optimizer=optimizer,\n                  save_dir=save_dir)\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------\n# Train\n#------------------------    \nprint('training process...')\nexpt.train(data_dir=data_dir, band_ind=band_ind, \n           epochs=epochs, resume=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------\n# Test\n#------------------------    \n# Evaluation\nprint('evaluation process...')\nexpt.test(data_dir=data_dir, subUL=subUL, \n          band_ind=band_ind, block_size=block_size)  # lr_block_size=lr_block_size\n#    for test_set in param['test_sets']:\n#        expt.test(test_set=test_set, lr_block_size=lr_block_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}